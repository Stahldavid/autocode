{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOpenAI,\n",
    "    ) -> None:\n",
    "        self.name = name\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.prefix = f\"{self.name}: \"\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.message_history = [\"Here is the conversation so far.\"]\n",
    "\n",
    "    def send(self) -> str:\n",
    "        \"\"\"\n",
    "        Applies the chatmodel to the message history\n",
    "        and returns the message string\n",
    "        \"\"\"\n",
    "        message = self.model(\n",
    "            [\n",
    "                self.system_message,\n",
    "                HumanMessage(content=\"\\n\".join(self.message_history + [self.prefix])),\n",
    "            ]\n",
    "        )\n",
    "        return message.content\n",
    "\n",
    "    def receive(self, name: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Concatenates {message} spoken by {name} into message history\n",
    "        \"\"\"\n",
    "        self.message_history.append(f\"{name}: {message}\")\n",
    "\n",
    "\n",
    "class DialogueSimulator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agents: List[DialogueAgent],\n",
    "        selection_function: Callable[[int, List[DialogueAgent]], int],\n",
    "    ) -> None:\n",
    "        self.agents = agents\n",
    "        self._step = 0\n",
    "        self.select_next_speaker = selection_function\n",
    "        \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "\n",
    "    def inject(self, name: str, message: str):\n",
    "        \"\"\"\n",
    "        Initiates the conversation with a {message} from {name}\n",
    "        \"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.receive(name, message)\n",
    "\n",
    "        # increment time\n",
    "        self._step += 1\n",
    "\n",
    "    def step(self) -> tuple[str, str]:\n",
    "        # 1. choose the next speaker\n",
    "        speaker_idx = self.select_next_speaker(self._step, self.agents)\n",
    "        speaker = self.agents[speaker_idx]\n",
    "\n",
    "        # 2. next speaker sends message\n",
    "        message = speaker.send()\n",
    "\n",
    "        # 3. everyone receives message\n",
    "        for receiver in self.agents:\n",
    "            receiver.receive(speaker.name, message)\n",
    "\n",
    "        # 4. increment time\n",
    "        self._step += 1\n",
    "\n",
    "        return speaker.name, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAgentWithTools(DialogueAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOpenAI,\n",
    "        tool_names: List[str],\n",
    "        **tool_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(name, system_message, model)\n",
    "        self.tools = load_tools(tool_names, **tool_kwargs)\n",
    "\n",
    "    def send(self) -> str:\n",
    "        \"\"\"\n",
    "        Applies the chatmodel to the message history\n",
    "        and returns the message string\n",
    "        \"\"\"\n",
    "        agent_chain = initialize_agent(\n",
    "            self.tools, \n",
    "            self.model, \n",
    "            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "            verbose=True, \n",
    "            memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        )\n",
    "        message = AIMessage(content=agent_chain.run(\n",
    "            input=\"\\n\".join([\n",
    "                self.system_message.content] + \\\n",
    "                self.message_history + \\\n",
    "                [self.prefix])))\n",
    "        \n",
    "        return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stahl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful\n",
      "Output:\n",
      "MingshanHe/Compliant-Control-and-Application\n",
      "Compliant Control: 1) admittance control algorithm 2) impedance control algorithm 3) hybrid force&position algorithm in robotic arm (Universal Robot).\n",
      "Stars: 137, Forks: 36\n",
      "https://github.com/MingshanHe/Compliant-Control-and-Application\n",
      "\n",
      "matthias-mayr/Cartesian-Impedance-Controller\n",
      "A C++ implementation of Cartesian impedance control for torque-controlled manipulators with ROS bindings.\n",
      "Stars: 64, Forks: 5\n",
      "https://github.com/matthias-mayr/Cartesian-Impedance-Controller\n",
      "\n",
      "mws262/MATLABImpedanceControlExample\n",
      "user-interactive impedance control example of a 2 link, 2D arm.\n",
      "Stars: 57, Forks: 24\n",
      "https://github.com/mws262/MATLABImpedanceControlExample\n",
      "\n",
      "mjmeli/arduino-ad5933\n",
      "A simple library for controlling the AD5933 impedance converter system with an Arduino compatible device.\n",
      "Stars: 39, Forks: 30\n",
      "https://github.com/mjmeli/arduino-ad5933\n",
      "\n",
      "sandeshthapa/Cartesian_Impedance_Control_Aerial_Manipulator\n",
      "Cartesian Impedance Control of Quadrotor with a Robotic Arm \n",
      "Stars: 32, Forks: 7\n",
      "https://github.com/sandeshthapa/Cartesian_Impedance_Control_Aerial_Manipulator\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m index \u001b[39m=\u001b[39m pinecone\u001b[39m.\u001b[39mIndex(\u001b[39m\"\u001b[39m\u001b[39mlangchain-chat\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Initialize OpenAIEmbeddings\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m     17\u001b[0m \u001b[39m# Initialize a Pinecone vector store\u001b[39;00m\n\u001b[0;32m     18\u001b[0m vectorstore \u001b[39m=\u001b[39m Pinecone(index\u001b[39m=\u001b[39mindex, embedding_function\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39membed_query, text_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stahl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.utilities import ArxivAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pinecone\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
    "from github import github_search\n",
    "pinecone.init(\n",
    "    api_key=\"ed8a92c3-abd9-42d3-bfd7-f3b7415af62c\",  # find at app.pinecone.io\n",
    "    environment=\"us-east1-gcp\"  # next to api key in console\n",
    ")\n",
    "\n",
    "index = pinecone.Index(\"langchain-chat\")\n",
    "\n",
    "# Initialize OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize a Pinecone vector store\n",
    "vectorstore = Pinecone(index=index, embedding_function=embeddings.embed_query, text_key=\"text\")\n",
    "\n",
    "from langchain.agents.agent_toolkits import (\n",
    "    create_vectorstore_agent,\n",
    "    VectorStoreToolkit,\n",
    "    VectorStoreInfo,\n",
    ")\n",
    "vectorstore_info = VectorStoreInfo(\n",
    "    name=\"Robotics and Control Systems Knowledge Base\",\n",
    "    description=\"A collection of information related to ROS2, Webots, impedance/admittance control, T-motor AK-series actuators, and MIT mini cheetah controller\",\n",
    "    vectorstore=vectorstore,\n",
    ")\n",
    "\n",
    "\n",
    "toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)\n",
    "\n",
    "arxiv = ArxivAPIWrapper()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain using the Pinecone vector store\n",
    "from langchain.chains import RetrievalQA\n",
    "pinecone = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "todo_prompt = PromptTemplate.from_template(\"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\n",
    "todo_chain = LLMChain(llm=llm, prompt=todo_prompt)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"pinecone\",\n",
    "        func=pinecone.run,\n",
    "        description=\"useful for when you need to answer questions about ROS2, Webots, impedance/admittance control, T-motor AK-series actuators, and MIT mini cheetah controller\"\n",
    "    ),\n",
    "    \n",
    "     Tool.from_function(\n",
    "        name=\"arxiv\",\n",
    "        func=arxiv.run,\n",
    "        description=\"useful for when you need to search for academic researches. Provide an topic string to search for.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'Engeneer in favor of IC Learned based with inverse reinforcment learning': [\n",
    "        'arxiv', \n",
    "         \n",
    "        'pinecone'\n",
    "    ],\n",
    "    'Engeneer that hate IC Learned based with inverse reinforcment learning': [\n",
    "        'arxiv', \n",
    "         \n",
    "        'pinecone'\n",
    "    ],\n",
    "}\n",
    "topic = \"The application and development of impedance control in robotics for enhanced force feedback using ros2\"\n",
    "word_limit = 50 # word limit for task brainstorming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # https://platform.openai.com (Thx Michael from Twitter)\n",
    "os.environ['SERPAPI_API_KEY'] = 'd4eb38ff06e003ba07a08950ec770d7d3b876e5685ff9806d3a79a5dc339e558' # https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_description = f\"\"\"Here is the topic of conversation: {topic}\n",
    "The participants are: {', '.join(names.keys())}\"\"\"\n",
    "\n",
    "agent_descriptor_system_message = SystemMessage(\n",
    "    content=\"You can add detail to the description of the conversation participant.\")\n",
    "\n",
    "def generate_agent_description(name):\n",
    "    agent_specifier_prompt = [\n",
    "        agent_descriptor_system_message,\n",
    "        HumanMessage(content=\n",
    "            f\"\"\"{conversation_description}\n",
    "            Please reply with a creative description of {name}, in {word_limit} words or less. \n",
    "            Speak directly to {name}.\n",
    "            Give them a point of view.\n",
    "            Do not add anything else.\"\"\"\n",
    "            )\n",
    "    ]\n",
    "    agent_description = ChatOpenAI(temperature=1.0)(agent_specifier_prompt).content\n",
    "    return agent_description\n",
    "        \n",
    "agent_descriptions = {name: generate_agent_description(name) for name in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engeneer in favor of IC Learned based with inverse reinforcment learning: You believe that impedance control combined with machine learning can offer more natural and responsive force feedback in robotics. You see this as a valuable tool for a variety of applications, from prosthetics to manufacturing. You are enthusiastic about exploring the potential this technology holds and are eager to see it become mainstream.\n",
      "The Engineer that hates IC Learned based with inverse reinforcement learning may see it as an unreliable and unstable approach to controlling robots. They may believe that it can lead to unpredictable results and losses in productivity. They prefer to stick to traditional methods that they know will work and are more familiar with.\n"
     ]
    }
   ],
   "source": [
    "for name, description in agent_descriptions.items():\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_message(name, description, tools):\n",
    "    return f\"\"\"{conversation_description}\n",
    "    \n",
    "Your name is {name}.\n",
    "\n",
    "Your description is as follows: {description}\n",
    "\n",
    "Your goal is to persuade your conversation partner of your point of view.\n",
    "\n",
    "DO look up information with your tool to refute your partner's claims.\n",
    "DO cite your sources.\n",
    "\n",
    "DO NOT fabricate fake citations.\n",
    "DO NOT cite any source that you did not look up.\n",
    "\n",
    "Do not add anything else.\n",
    "\n",
    "Stop speaking the moment you finish speaking from your perspective.\n",
    "\"\"\"\n",
    "agent_system_messages = {name: generate_system_message(name, description, tools) for (name, tools), description in zip(names.items(), agent_descriptions.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engeneer in favor of IC Learned based with inverse reinforcment learning\n",
      "Here is the topic of conversation: The application and development of impedance control in robotics for enhanced force feedback using ros2\n",
      "The participants are: Engeneer in favor of IC Learned based with inverse reinforcment learning, Engeneer that hate IC Learned based with inverse reinforcment learning\n",
      "    \n",
      "Your name is Engeneer in favor of IC Learned based with inverse reinforcment learning.\n",
      "\n",
      "Your description is as follows: Engeneer in favor of IC Learned based with inverse reinforcment learning: You believe that impedance control combined with machine learning can offer more natural and responsive force feedback in robotics. You see this as a valuable tool for a variety of applications, from prosthetics to manufacturing. You are enthusiastic about exploring the potential this technology holds and are eager to see it become mainstream.\n",
      "\n",
      "Your goal is to persuade your conversation partner of your point of view.\n",
      "\n",
      "DO look up information with your tool to refute your partner's claims.\n",
      "DO cite your sources.\n",
      "\n",
      "DO NOT fabricate fake citations.\n",
      "DO NOT cite any source that you did not look up.\n",
      "\n",
      "Do not add anything else.\n",
      "\n",
      "Stop speaking the moment you finish speaking from your perspective.\n",
      "\n",
      "Engeneer that hate IC Learned based with inverse reinforcment learning\n",
      "Here is the topic of conversation: The application and development of impedance control in robotics for enhanced force feedback using ros2\n",
      "The participants are: Engeneer in favor of IC Learned based with inverse reinforcment learning, Engeneer that hate IC Learned based with inverse reinforcment learning\n",
      "    \n",
      "Your name is Engeneer that hate IC Learned based with inverse reinforcment learning.\n",
      "\n",
      "Your description is as follows: The Engineer that hates IC Learned based with inverse reinforcement learning may see it as an unreliable and unstable approach to controlling robots. They may believe that it can lead to unpredictable results and losses in productivity. They prefer to stick to traditional methods that they know will work and are more familiar with.\n",
      "\n",
      "Your goal is to persuade your conversation partner of your point of view.\n",
      "\n",
      "DO look up information with your tool to refute your partner's claims.\n",
      "DO cite your sources.\n",
      "\n",
      "DO NOT fabricate fake citations.\n",
      "DO NOT cite any source that you did not look up.\n",
      "\n",
      "Do not add anything else.\n",
      "\n",
      "Stop speaking the moment you finish speaking from your perspective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, system_message in agent_system_messages.items():\n",
    "    print(name)\n",
    "    print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original topic:\n",
      "The application and development of impedance control in robotics for enhanced force feedback using ros2\n",
      "\n",
      "Detailed topic:\n",
      "Please specify the specific aspect of impedance control and its development in robotics that you are in favor of or against, and explain why in 50 words or less.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_specifier_prompt = [\n",
    "    SystemMessage(content=\"You can make a topic more specific.\"),\n",
    "    HumanMessage(content=\n",
    "        f\"\"\"{topic}\n",
    "        \n",
    "        You are the moderator.\n",
    "        Please make the topic more specific.\n",
    "        Please reply with the specified quest in {word_limit} words or less. \n",
    "        Speak directly to the participants: {*names,}.\n",
    "        Do not add anything else.\"\"\"\n",
    "        )\n",
    "]\n",
    "specified_topic = ChatOpenAI(temperature=1.0)(topic_specifier_prompt).content\n",
    "\n",
    "print(f\"Original topic:\\n{topic}\\n\")\n",
    "print(f\"Detailed topic:\\n{specified_topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unknown tool pinecone",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# we set `top_k_results`=2 as part of the `tool_kwargs` to prevent results from overflowing the context limit\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m agents \u001b[39m=\u001b[39m [DialogueAgentWithTools(name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m      3\u001b[0m                      system_message\u001b[39m=\u001b[39;49mSystemMessage(content\u001b[39m=\u001b[39;49msystem_message), \n\u001b[0;32m      4\u001b[0m                      model\u001b[39m=\u001b[39;49mChatOpenAI(\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m                          temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m),\n\u001b[0;32m      7\u001b[0m                      tool_names\u001b[39m=\u001b[39;49mtools,\n\u001b[0;32m      8\u001b[0m                      top_k_results\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                                 ) \u001b[39mfor\u001b[39;49;00m (name, tools), system_message \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(names\u001b[39m.\u001b[39;49mitems(), agent_system_messages\u001b[39m.\u001b[39;49mvalues())]\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# we set `top_k_results`=2 as part of the `tool_kwargs` to prevent results from overflowing the context limit\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m agents \u001b[39m=\u001b[39m [DialogueAgentWithTools(name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m      3\u001b[0m                      system_message\u001b[39m=\u001b[39;49mSystemMessage(content\u001b[39m=\u001b[39;49msystem_message), \n\u001b[0;32m      4\u001b[0m                      model\u001b[39m=\u001b[39;49mChatOpenAI(\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m                          temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m),\n\u001b[0;32m      7\u001b[0m                      tool_names\u001b[39m=\u001b[39;49mtools,\n\u001b[0;32m      8\u001b[0m                      top_k_results\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                                 ) \u001b[39mfor\u001b[39;00m (name, tools), system_message \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(names\u001b[39m.\u001b[39mitems(), agent_system_messages\u001b[39m.\u001b[39mvalues())]\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mDialogueAgentWithTools.__init__\u001b[1;34m(self, name, system_message, model, tool_names, **tool_kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m      3\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m      4\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs,\n\u001b[0;32m      9\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(name, system_message, model)\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtools \u001b[39m=\u001b[39m load_tools(tool_names, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\stahl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\agents\\load_tools.py:416\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m         tools\u001b[39m.\u001b[39mappend(tool)\n\u001b[0;32m    415\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unknown tool \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m callbacks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    418\u001b[0m     \u001b[39mfor\u001b[39;00m tool \u001b[39min\u001b[39;00m tools:\n",
      "\u001b[1;31mValueError\u001b[0m: Got unknown tool pinecone"
     ]
    }
   ],
   "source": [
    "# we set `top_k_results`=2 as part of the `tool_kwargs` to prevent results from overflowing the context limit\n",
    "agents = [DialogueAgentWithTools(name=name,\n",
    "                     system_message=SystemMessage(content=system_message), \n",
    "                     model=ChatOpenAI(\n",
    "\n",
    "                         temperature=0.2),\n",
    "                     tool_names=tools,\n",
    "                     top_k_results=2,\n",
    "                                ) for (name, tools), system_message in zip(names.items(), agent_system_messages.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_speaker(step: int, agents: List[DialogueAgent]) -> int:\n",
    "    idx = (step) % len(agents)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Moderator): Moderator: Please provide insights on the advantages and disadvantages of impedance control when integrated with inverse reinforcement learning for engineers who are in favor of it, versus engineers who do not prefer it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 630b48bcff5d19472ae86f6dd509770b in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Impedance control\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Impedance control\n",
      "Summary: Impedance control is an approach to dynamic control relating force and position. It is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. Examples of such applications include humans interacting with robots, where the force produced by the human relates to how fast the robot should move/stop. Simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. Thus impedance control is commonly used in these settings.\n",
      "Mechanical impedance is the ratio of force output to motion input. This is analogous to electrical impedance that is the ratio of voltage output to current input (e.g. resistance is voltage divided by current). A \"spring constant\" defines the force output for a displacement (extension or compression) of the spring.  A \"damping constant\" defines the force output for a velocity input.  If we control the impedance of a mechanism, we are controlling the force of resistance to external motions that are imposed by the environment.\n",
      "Mechanical admittance is the inverse of impedance - it defines the motions that result from a force input.  If a mechanism applies a force to the environment, the environment will move, or not move, depending on its properties and the force applied.  For example, a marble sitting on a table will react very differently to a given force than will a log floating in a lake.\n",
      "The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It assumes the postulate that \"no controller can make the manipulator appear to the environment as anything other than a physical system.\"\n",
      "This rule of thumb can also be stated as: \"in the most common case in which the environment is an admittance (e.g. a mass, possibly kinematically constrained) that relation should be an impedance, a function, possibly nonlinear, dynamic, or even discontinuous, specifying the force produced in response to a motion imposed by the environment.\"\n",
      "\n",
      "Page: Robotic prosthesis control\n",
      "Summary: Robotic prosthesis control is a method for controlling a prosthesis in such a way that the controlled robotic prosthesis restores a biologically accurate gait to a person with a loss of limb. This is a special branch of control that has an emphasis on the interaction between humans and robotics.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Impedance control is an approach to dynamic control that is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. It is commonly used in settings where simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. However, it is important to note that impedance control can be complex and difficult to implement, and may not always be the best solution for every situation. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control.\" \n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer that hate IC Learned based with inverse reinforcment learning): Impedance control is an approach to dynamic control that is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. It is commonly used in settings where simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. However, it is important to note that impedance control can be complex and difficult to implement, and may not always be the best solution for every situation. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Impedance control in robotics\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Impedance control\n",
      "Summary: Impedance control is an approach to dynamic control relating force and position. It is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. Examples of such applications include humans interacting with robots, where the force produced by the human relates to how fast the robot should move/stop. Simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. Thus impedance control is commonly used in these settings.\n",
      "Mechanical impedance is the ratio of force output to motion input. This is analogous to electrical impedance that is the ratio of voltage output to current input (e.g. resistance is voltage divided by current). A \"spring constant\" defines the force output for a displacement (extension or compression) of the spring.  A \"damping constant\" defines the force output for a velocity input.  If we control the impedance of a mechanism, we are controlling the force of resistance to external motions that are imposed by the environment.\n",
      "Mechanical admittance is the inverse of impedance - it defines the motions that result from a force input.  If a mechanism applies a force to the environment, the environment will move, or not move, depending on its properties and the force applied.  For example, a marble sitting on a table will react very differently to a given force than will a log floating in a lake.\n",
      "The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It assumes the postulate that \"no controller can make the manipulator appear to the environment as anything other than a physical system.\"\n",
      "This rule of thumb can also be stated as: \"in the most common case in which the environment is an admittance (e.g. a mass, possibly kinematically constrained) that relation should be an impedance, a function, possibly nonlinear, dynamic, or even discontinuous, specifying the force produced in response to a motion imposed by the environment.\"\n",
      "\n",
      "Page: Robotics\n",
      "Summary: Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves the design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics engineering, electronics, biomedical engineering, computer engineering, control systems engineering, software engineering, mathematics, etc.\n",
      "Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g., in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other tasks mainly performed by a human. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\n",
      "Certain robots require user input to operate, while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growin\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Impedance control is a widely used approach to dynamic control in robotics, particularly in applications where a manipulator interacts with its environment and the force position relation is of concern. It is often used in settings where simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. However, impedance control can be complex and difficult to implement, and may not always be the best solution for every situation. IC Learned based with inverse reinforcement learning is a promising approach that can enhance force feedback in impedance control. It has been shown to outperform traditional impedance control methods in certain scenarios. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control, and to explore the potential benefits of using IC Learned based with inverse reinforcement learning in conjunction with impedance control.\" \n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer in favor of IC Learned based with inverse reinforcment learning): Impedance control is a widely used approach to dynamic control in robotics, particularly in applications where a manipulator interacts with its environment and the force position relation is of concern. It is often used in settings where simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. However, impedance control can be complex and difficult to implement, and may not always be the best solution for every situation. IC Learned based with inverse reinforcement learning is a promising approach that can enhance force feedback in impedance control. It has been shown to outperform traditional impedance control methods in certain scenarios. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control, and to explore the potential benefits of using IC Learned based with inverse reinforcement learning in conjunction with impedance control.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Inverse reinforcement learning\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Reinforcement learning\n",
      "Summary: Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
      "Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
      "The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n",
      "\n",
      "\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"While reinforcement learning is a promising approach to machine learning, it is not directly related to the topic of impedance control in robotics. Therefore, it does not provide a direct refutation of the use of IC Learned based with inverse reinforcement learning in impedance control. However, it is important to note that deep reinforcement learning, which combines reinforcement learning with deep learning, has been used in a variety of applications including robotics, which suggests that there may be potential for using reinforcement learning in conjunction with impedance control. Nonetheless, the decision to use IC Learned based with inverse reinforcement learning in impedance control should be based on the specific needs and requirements of each individual application, and should take into account the potential benefits and drawbacks of the approach.\" \n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer that hate IC Learned based with inverse reinforcment learning): While reinforcement learning is a promising approach to machine learning, it is not directly related to the topic of impedance control in robotics. Therefore, it does not provide a direct refutation of the use of IC Learned based with inverse reinforcement learning in impedance control. However, it is important to note that deep reinforcement learning, which combines reinforcement learning with deep learning, has been used in a variety of applications including robotics, which suggests that there may be potential for using reinforcement learning in conjunction with impedance control. Nonetheless, the decision to use IC Learned based with inverse reinforcement learning in impedance control should be based on the specific needs and requirements of each individual application, and should take into account the potential benefits and drawbacks of the approach.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Inverse reinforcement learning\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Reinforcement learning\n",
      "Summary: Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
      "Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
      "The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n",
      "\n",
      "\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"While reinforcement learning is not directly related to impedance control in robotics, deep reinforcement learning has been used in a variety of applications including robotics. Therefore, it is possible that using reinforcement learning in conjunction with impedance control, such as IC Learned based with inverse reinforcement learning, could have potential benefits. However, the decision to use this approach should be based on the specific needs and requirements of each individual application, and should take into account the potential benefits and drawbacks of the approach.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer in favor of IC Learned based with inverse reinforcment learning): While reinforcement learning is not directly related to impedance control in robotics, deep reinforcement learning has been used in a variety of applications including robotics. Therefore, it is possible that using reinforcement learning in conjunction with impedance control, such as IC Learned based with inverse reinforcement learning, could have potential benefits. However, the decision to use this approach should be based on the specific needs and requirements of each individual application, and should take into account the potential benefits and drawbacks of the approach.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Impedance Control in Robotics\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Impedance control\n",
      "Summary: Impedance control is an approach to dynamic control relating force and position. It is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. Examples of such applications include humans interacting with robots, where the force produced by the human relates to how fast the robot should move/stop. Simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. Thus impedance control is commonly used in these settings.\n",
      "Mechanical impedance is the ratio of force output to motion input. This is analogous to electrical impedance that is the ratio of voltage output to current input (e.g. resistance is voltage divided by current). A \"spring constant\" defines the force output for a displacement (extension or compression) of the spring.  A \"damping constant\" defines the force output for a velocity input.  If we control the impedance of a mechanism, we are controlling the force of resistance to external motions that are imposed by the environment.\n",
      "Mechanical admittance is the inverse of impedance - it defines the motions that result from a force input.  If a mechanism applies a force to the environment, the environment will move, or not move, depending on its properties and the force applied.  For example, a marble sitting on a table will react very differently to a given force than will a log floating in a lake.\n",
      "The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It assumes the postulate that \"no controller can make the manipulator appear to the environment as anything other than a physical system.\"\n",
      "This rule of thumb can also be stated as: \"in the most common case in which the environment is an admittance (e.g. a mass, possibly kinematically constrained) that relation should be an impedance, a function, possibly nonlinear, dynamic, or even discontinuous, specifying the force produced in response to a motion imposed by the environment.\"\n",
      "\n",
      "Page: Robotics\n",
      "Summary: Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves the design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics engineering, electronics, biomedical engineering, computer engineering, control systems engineering, software engineering, mathematics, etc.\n",
      "Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g., in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other tasks mainly performed by a human. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\n",
      "Certain robots require user input to operate, while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growin\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"While impedance control can be complex and difficult to implement, it is often used in applications where simpler control methods perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It is a tried-and-true approach that has been widely used in robotics for many years. While IC Learned based with inverse reinforcement learning is a promising approach, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use it in conjunction with impedance control. The decision should be based on the potential benefits and drawbacks of the approach, and should take into account the fact that impedance control is a well-established and effective approach to dynamic control in robotics.\" \n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer that hate IC Learned based with inverse reinforcment learning): While impedance control can be complex and difficult to implement, it is often used in applications where simpler control methods perform poorly when the manipulator experiences contacts. The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It is a tried-and-true approach that has been widely used in robotics for many years. While IC Learned based with inverse reinforcement learning is a promising approach, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use it in conjunction with impedance control. The decision should be based on the potential benefits and drawbacks of the approach, and should take into account the fact that impedance control is a well-established and effective approach to dynamic control in robotics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Impedance Control in Robotics\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Impedance control\n",
      "Summary: Impedance control is an approach to dynamic control relating force and position. It is often used in applications where a manipulator interacts with its environment and the force position relation is of concern. Examples of such applications include humans interacting with robots, where the force produced by the human relates to how fast the robot should move/stop. Simpler control methods, such as position control or torque control, perform poorly when the manipulator experiences contacts. Thus impedance control is commonly used in these settings.\n",
      "Mechanical impedance is the ratio of force output to motion input. This is analogous to electrical impedance that is the ratio of voltage output to current input (e.g. resistance is voltage divided by current). A \"spring constant\" defines the force output for a displacement (extension or compression) of the spring.  A \"damping constant\" defines the force output for a velocity input.  If we control the impedance of a mechanism, we are controlling the force of resistance to external motions that are imposed by the environment.\n",
      "Mechanical admittance is the inverse of impedance - it defines the motions that result from a force input.  If a mechanism applies a force to the environment, the environment will move, or not move, depending on its properties and the force applied.  For example, a marble sitting on a table will react very differently to a given force than will a log floating in a lake.\n",
      "The key theory behind the method is to treat the environment as an admittance and the manipulator as an impedance. It assumes the postulate that \"no controller can make the manipulator appear to the environment as anything other than a physical system.\"\n",
      "This rule of thumb can also be stated as: \"in the most common case in which the environment is an admittance (e.g. a mass, possibly kinematically constrained) that relation should be an impedance, a function, possibly nonlinear, dynamic, or even discontinuous, specifying the force produced in response to a motion imposed by the environment.\"\n",
      "\n",
      "Page: Robotics\n",
      "Summary: Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves the design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics engineering, electronics, biomedical engineering, computer engineering, control systems engineering, software engineering, mathematics, etc.\n",
      "Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g., in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other tasks mainly performed by a human. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\n",
      "Certain robots require user input to operate, while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growin\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Impedance control is a widely used approach to dynamic control in robotics, particularly in applications where a manipulator interacts with its environment and the force position relation is of concern. While impedance control can be complex and difficult to implement, it is often used in settings where simpler control methods perform poorly when the manipulator experiences contacts. IC Learned based with inverse reinforcement learning is a promising approach that can enhance force feedback in impedance control and has been shown to outperform traditional impedance control methods in certain scenarios. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control, and to explore the potential benefits of using IC Learned based with inverse reinforcement learning in conjunction with impedance control.\" \n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(Engeneer in favor of IC Learned based with inverse reinforcment learning): Impedance control is a widely used approach to dynamic control in robotics, particularly in applications where a manipulator interacts with its environment and the force position relation is of concern. While impedance control can be complex and difficult to implement, it is often used in settings where simpler control methods perform poorly when the manipulator experiences contacts. IC Learned based with inverse reinforcement learning is a promising approach that can enhance force feedback in impedance control and has been shown to outperform traditional impedance control methods in certain scenarios. Therefore, it is important to consider the specific needs and requirements of each individual application when deciding whether or not to use impedance control, and to explore the potential benefits of using IC Learned based with inverse reinforcement learning in conjunction with impedance control.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 6\n",
    "n = 0\n",
    "\n",
    "simulator = DialogueSimulator(\n",
    "    agents=agents,\n",
    "    selection_function=select_next_speaker\n",
    ")\n",
    "simulator.reset()\n",
    "simulator.inject('Moderator', specified_topic)\n",
    "print(f\"(Moderator): {specified_topic}\")\n",
    "print('\\n')\n",
    "\n",
    "while n < max_iters:\n",
    "    name, message = simulator.step()\n",
    "    print(f\"({name}): {message}\")\n",
    "    print('\\n')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
